{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crane_path = r'/work/cseos2g/datduyn/Documents/datasets/wine-quality/winequality-white.csv'\n",
    "window_path = r'C:\\Users\\datng\\Documents\\datasets\\wine-quality\\winequality-white.csv'\n",
    "#crane_path = r'/home/cse496dl/otiong/machineLearning/assignment_1/winequality-white.csv'\n",
    "# window_path = r'C:\\Users\\onsai\\CSCE478\\datasets\\DataScienceRepository\\winequality-white.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(window_path,delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\datng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\datng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3258\n",
       "0    1640\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure you run this line only once\n",
    "df['quality'][df['quality'] <= 5] = 0 # Bad wine\n",
    "df['quality'][df['quality'] > 5] = 1 # Good wine\n",
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure to run this cell once\n",
    "df = df.dropna()\n",
    "df = df.drop(columns=['fixed acidity','residual sugar'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the most correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\datng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.34</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.9940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10.1</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.40</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.9951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.32</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.9956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  alcohol    pH  sulphates  citric acid  free sulfur dioxide  \\\n",
       "0        1      8.8  3.00       0.45         0.36                 45.0   \n",
       "1        1      9.5  3.30       0.49         0.34                 14.0   \n",
       "2        1     10.1  3.26       0.44         0.40                 30.0   \n",
       "3        1      9.9  3.19       0.40         0.32                 47.0   \n",
       "4        1      9.9  3.19       0.40         0.32                 47.0   \n",
       "\n",
       "   total sulfur dioxide  chlorides  volatile acidity  density  \n",
       "0                 170.0      0.045              0.27   1.0010  \n",
       "1                 132.0      0.049              0.30   0.9940  \n",
       "2                  97.0      0.050              0.28   0.9951  \n",
       "3                 186.0      0.058              0.23   0.9956  \n",
       "4                 186.0      0.058              0.23   0.9956  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this cell only once\n",
    "most_corr = df.corr()['quality'].sort_values(ascending=False)\n",
    "df = df.ix[:, most_corr.index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Partition the data into train and test set\n",
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4898, 9)\n",
      "y shape: (4898,)\n"
     ]
    }
   ],
   "source": [
    "y = df['quality']\n",
    "X = df.drop('quality', axis=1)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 4898\n",
      "X_train (3918, 9)\n",
      "y_train (3918,)\n",
      "X_test (980, 9)\n",
      "y_test (980,)\n",
      "X_dev (10, 9)\n",
      "y_dev (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\datng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "c:\\users\\datng\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "def split_train_test(X, y, \n",
    "                     partition_percent=[.8,.2], shuffle=False):\n",
    "    '''\n",
    "    Partioning the data:\n",
    "    args:\n",
    "        - X: train set numpy or dataframe with shape(N,D)\n",
    "        - y: labels numpy or dataframe with shape(N,)\n",
    "        - partition_percent: python list of data percentage\n",
    "         to partition. ex: [.8, .2] -> part of data\n",
    "         will be split to .8 and part will be .2.\n",
    "         NOTE: have to sum up to 1.0\n",
    "        - shuffle: shuffle before partitioning or not?\n",
    "    '''\n",
    "    \n",
    "    #convert from panda series to numpy\n",
    "    try:\n",
    "        X = X.as_matrix()\n",
    "        y = y.as_matrix()\n",
    "    except: #incase given X, y is already a numpy arra\n",
    "        pass\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    if(sum(partition_percent) != 1.0):\n",
    "        raise ValueError(\"Partition_percent should add up to 1.0\")\n",
    "    if shuffle:\n",
    "        X = np.random.shuffle(X)\n",
    "    \n",
    "    split_sets = []\n",
    "    s = [] #[X1,y1, X2, y2, X3, y3...]\n",
    "    prev_idx = 0\n",
    "    for idx,part in enumerate(partition_percent):\n",
    "        range_idx = int(N * part)\n",
    "        if(idx == len(partition_percent)-1):\n",
    "            split_sets.append(X[prev_idx:])\n",
    "            split_sets.append(y[prev_idx:])\n",
    "        else:\n",
    "            split_sets.append(X[prev_idx:prev_idx+range_idx])\n",
    "            split_sets.append(y[prev_idx:prev_idx+range_idx])\n",
    "        prev_idx = prev_idx+range_idx\n",
    "        \n",
    "    return split_sets\n",
    "\n",
    "\n",
    "print('X', len(X))\n",
    "X_train, y_train, X_test, y_test= split_train_test(X, y, \n",
    "                partition_percent=[.8,.2], \n",
    "                shuffle=False)\n",
    "\n",
    "#create dev set for debuging\n",
    "X_dev = X_train[0:10].copy()\n",
    "y_dev = y_train[0:10].copy()\n",
    "\n",
    "# Check shape\n",
    "print(\"X_train\",X_train.shape)\n",
    "print(\"y_train\",y_train.shape)\n",
    "print(\"X_test\",X_test.shape)\n",
    "print(\"y_test\",y_test.shape)\n",
    "\n",
    "print(\"X_dev\",X_dev.shape)\n",
    "print(\"y_dev\",y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.90234465e-13 1.05647588e+00 1.49743452e+00 2.39125446e+00\n",
      " 2.88910586e+00 3.02551287e+00 3.33277635e+00 3.64294426e+00\n",
      " 3.88016965e+00 4.06421475e+00 4.37772143e+00 4.62125335e+00\n",
      " 4.79915716e+00 4.98573493e+00 5.12999348e+00 5.32476289e+00\n",
      " 5.56245127e+00 5.73226622e+00 5.88773679e+00 6.05570512e+00\n",
      " 6.24229196e+00 6.38323263e+00 6.55937844e+00 6.77073589e+00\n",
      " 6.82631682e+00 6.98051166e+00 7.14916350e+00 7.28422402e+00\n",
      " 7.39666471e+00 7.48327885e+00 7.60771521e+00 7.72024066e+00\n",
      " 7.91425200e+00 8.02866213e+00 8.17211004e+00 8.25260090e+00\n",
      " 8.33673099e+00 8.37120060e+00 8.46108162e+00 8.60060579e+00\n",
      " 8.72189325e+00 8.88141863e+00 8.94571249e+00 9.04048900e+00\n",
      " 9.14763654e+00 9.25079473e+00 9.36100427e+00 9.46187021e+00\n",
      " 9.54652782e+00 9.64531177e+00 9.73687514e+00 9.80596825e+00\n",
      " 9.93746092e+00 1.00691090e+01 1.01287879e+01 1.02069614e+01\n",
      " 1.03063650e+01 1.03751300e+01 1.04646581e+01 1.05382883e+01\n",
      " 1.06134996e+01 1.06738064e+01 1.07443450e+01 1.08339053e+01\n",
      " 1.09287814e+01 1.10378542e+01 1.11169521e+01 1.11834324e+01\n",
      " 1.12563832e+01 1.13478544e+01 1.14211928e+01 1.14924368e+01\n",
      " 1.15780643e+01 1.16456632e+01 1.16971040e+01 1.17435511e+01\n",
      " 1.17945587e+01 1.18371477e+01 1.18680085e+01] [ 2.25327988  3.27060146  3.77525231  3.70280586  3.61085662  4.14939301\n",
      "  4.22902569  4.29512281  4.57174375  4.89377122  5.13092693  5.24551551\n",
      "  5.36704488  5.58526359  5.72314371  5.85084717  6.03417771  6.12852313\n",
      "  6.27283541  6.54563541  6.60805008  6.79153642  6.96204397  7.02572463\n",
      "  7.18209951  7.36638436  7.52223187  7.5747239   7.70382405  7.80155558\n",
      "  7.87089457  7.96540208  8.04456381  8.22830337  8.32053602  8.46980463\n",
      "  8.59017396  8.76957174  8.91747949  9.03841504  9.16077479  9.22927635\n",
      "  9.36109404  9.40044447  9.52336072  9.61192606  9.74554853  9.87749929\n",
      "  9.9863168  10.02675648 10.06886118 10.17860469 10.2451281  10.29663344\n",
      " 10.39130699 10.53561154 10.60066858 10.68353799 10.77013785 10.82449091\n",
      " 10.94266288 11.06171686 11.16706185 11.2336701  11.25991163 11.33074483\n",
      " 11.37777525 11.41676794 11.48933751 11.53441226 11.5772707  11.66149937\n",
      " 11.69794099 11.73809745 11.85501632 11.95239068 12.04851651 12.14852177\n",
      " 12.16161878]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def polynomialFeatures(X, degree):\n",
    "    '''\n",
    "    Arguments:\n",
    "\n",
    "    X : ndarray \n",
    "        A numpy array with rows representing data samples and columns representing features (d-dimensional feature).\n",
    "\n",
    "    degree : integer\n",
    "        The degree of the polynomial features. Default = 1.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        A new feature matrix consisting of all polynomial combinations of the features with degree equal to the specified degree. \n",
    "        For example, if an input sample is two dimensional and of the form [a, b], \n",
    "        the degree-2 polynomial features are [a, b, a2, ab, b2].\n",
    "    '''\n",
    "    map_term = {}\n",
    "    poly_term = []\n",
    "    result_term = []\n",
    "    for i in range(X.shape[1]):\n",
    "        map_terms[i] = X[:,i]\n",
    "        poly_term.append(i)\n",
    "\n",
    "    terms = get_polynomial(poly_term, degree)\n",
    "    result = 1\n",
    "    for sublst in terms:\n",
    "        for x in sublst:\n",
    "            if x == 'cons':\n",
    "                result *= np.ones((X.shape[0],), dtype=int)\n",
    "            else:\n",
    "                result *= map_terms[x] \n",
    "                \n",
    "        result_term.append(result)\n",
    "        result = 1\n",
    "    \n",
    "    return np.array(result_term).T\n",
    "\n",
    "def get_polynomial(vars, degree):\n",
    "\n",
    "    vars.append(\"cons\") # add dummy variable\n",
    "    \n",
    "    # compute all combinations of variables\n",
    "    terms = []\n",
    "    for x in itertools.combinations_with_replacement(vars, degree):\n",
    "        terms.append(x)\n",
    "\n",
    "    # get rid of \"c\" terms\n",
    "    terms = map(list, terms)\n",
    "    terms = list(terms)\n",
    " \n",
    "    return terms\n",
    "\n",
    "def mse(Y_true, Y_pred):\n",
    "    '''\n",
    "    Arguments: \n",
    "        Y_true : ndarray 1D array containing data with “float” type. True y values.\n",
    "        Y_pred : ndarray 1D array containing data with “float” type. Values predicted by your model.\n",
    "\n",
    "    Returns:\n",
    "        cost: float It returns a float value containing mean squared error between Y_true and Y_pred.\n",
    "\n",
    "    Note: these 1D arrays should be designed as column vectors.\n",
    "    '''\n",
    "    return sum((Y_true-Y_pred)**2)\n",
    "\n",
    "def learning_curve(model, X, Y, cv, train_size=1, learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0, **kwargs):\n",
    "    '''\n",
    "    Arguments: \n",
    "        model: object type that implements the “fit” and “predict” methods. An object of that type which is cloned for each validation.\n",
    "        \n",
    "        X: ndarray A numpy array with rows representing data samples and columns representing features.\n",
    "        \n",
    "        Y: ndarray A 1D numpy array with labels corresponding to each row of the feature matrix X.\n",
    "        \n",
    "        cv : int integer, to specify the number of folds in a k-fold cross-validation.\n",
    "        \n",
    "        train_sizes : intor float Relative or absolute numbers of training examples that will be used to generate the learning curve. \n",
    "            If the dtype is float, it is regarded as a fraction of the maximum size of the training set \n",
    "            (that is determined by the selected validation method), i.e. it has to be within (0, 1]. \n",
    "            Otherwise it is interpreted as absolute sizes of the training sets. \n",
    "        \n",
    "        learning_rate: float It provides the step size for parameter update.\n",
    "        \n",
    "        epochs :int The maximum number of passes over the training data for updating the weight vector.\n",
    "        \n",
    "        tol : float or None The stopping criterion. If it is not None, the iterations will stop when (error> previous_error-tol). \n",
    "            If it is None, the number of iterations will be set by the “epochs”.\n",
    "        \n",
    "        regularizer: string The string value could be one of the following: l1, l2, None. \n",
    "            If it’s set to None, the cost function without the regularization term will be used for computing the gradient \n",
    "            and updating the weight vector. However, if it’s set to l1 or l2, the appropriate regularized cost function needs to be used \n",
    "            for computing the gradient and updating the weight vector.\n",
    "        \n",
    "        lambd: floatIt provides the regularization coefficient. It is used only when the “regularizer” is set to l1 or l2.\n",
    "        \n",
    "    Returns:\n",
    "        train_scores : ndarray root-mean-square error(rmse) values on training sets.\n",
    "        \n",
    "        val_scores : ndarray root-mean-square error(rmse) values on validation sets.    \n",
    "    '''\n",
    "#edge case: value of cv must be more than sample size/ cannot be 1\n",
    "    num_iteration = math.ceil(X.shape[0]/train_size)\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    for i in range(1, num_iteration + 1):\n",
    "        cur_X = X[0:(i*train_size)]\n",
    "        cur_Y = Y[0:(i*train_size)]\n",
    "        x_train_folds = np.array(np.array_split(cur_X, cv))\n",
    "        y_train_folds = np.array(np.array_split(cur_Y, cv))\n",
    "        \n",
    "        train_score = 0\n",
    "        val_score = 0\n",
    "        for idx, val_fold in enumerate(x_train_folds):\n",
    "            train_folds = [f for i, f in enumerate(x_train_folds) if(i != idx)][0] #[0]: unpacking the outer list\n",
    "            label_folds = [f for i, f in enumerate(y_train_folds) if(i != idx)][0]\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(train_folds, label_folds)\n",
    "\n",
    "            train_pred = model.predict(train_folds)\n",
    "            val_pred = model.predict(val_fold)\n",
    "\n",
    "            train_score += (mse(label_folds, train_pred))\n",
    "            val_score += (mse(y_train_folds[idx], val_pred))\n",
    "            \n",
    "        train_scores.append(train_score/cv)\n",
    "        val_scores.append(val_score/cv)\n",
    "    \n",
    "    return np.sqrt(train_scores), np.sqrt(val_scores)\n",
    "    \n",
    "\n",
    "a, b =learning_curve(LinearRegression(), X_train, y_train, 5, train_size=50, learning_rate=0.01, epochs=1000, tol=None, regularizer=None, lambd=0.0)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, learning_rate=1e-2):\n",
    "    w -= learning_rate * dw\n",
    "    return w\n",
    "\n",
    "\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, optim=sgd):\n",
    "        self.optim = sgd\n",
    "        self.params = {} #save model weights\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.params['w']).reshape(-1) + self.params['b'] #(N,D) dot (D,1) = (N,1)\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        N, D = self.X.shape\n",
    "        grads = {}#return gradient for Gradient descent update\n",
    "        #forward pass\n",
    "        scores = self.predict(X)\n",
    "\n",
    "        #backward pass\n",
    "        loss, grads = 0.0, {}\n",
    "        \n",
    "        loss = (0.5/N) * np.sum((y-scores)**2)\n",
    "        #get mse loss and dout here\n",
    "        dout = (1/N) * (scores - y) \n",
    "        \n",
    "        #y = wx + b -> dy/db = dout/dy * dy/db = dout * 1.0\n",
    "        grads['b'] = np.sum(dout)\n",
    "        \n",
    "        #dout/dw = dout/dy dot dy/dw = dout dot x\n",
    "        #shape: (N,1) dot (N,D) -> X.T.dot(dout)\n",
    "        grads['w'] = dout.T.dot(X).reshape(-1, 1)\n",
    "\n",
    "        return loss, grads\n",
    "    \n",
    "    def fit(self, X, Y, learning_rate=1e-1, \n",
    "                   epochs=300, tol=1e-4, regularizer=0.0,\n",
    "                   lambd=None, verbose=False):\n",
    "        '''\n",
    "        Inputs:\n",
    "        - X: numpy array of inputs vector  (N, D)\n",
    "        - Y: numpy array of target vector (N,)\n",
    "        - learning_rate: float provide the step size\n",
    "        - epochs: int provide number of passes through in dataset\n",
    "        - tol: float or None provide the stopping criterion\n",
    "        - regularizer: string of 'l1' or 'l2'\n",
    "        - lambd: float provide regularize coeff\n",
    "        '''\n",
    "        loss_hist = []\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularizer = regularizer\n",
    "        self.tol = tol\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        N, D = self.X.shape\n",
    "        \n",
    "        self.params['w'] = np.random.randn(D,1)\n",
    "        self.params['b'] = 0.0\n",
    "        \n",
    "        for e in range(self.epochs):\n",
    "            loss, grads = self.loss(self.X, self.y)\n",
    "            \n",
    "            #perform gradient descent params update\n",
    "            for p, w in self.params.items():\n",
    "                dw = grads[p]\n",
    "                config={'learning_rate':self.learning_rate}\n",
    "                next_w = self.optim(w, dw, self.learning_rate)\n",
    "                self.params[p] = next_w\n",
    "                \n",
    "            if verbose: print('epoch', e, 'loss', loss)\n",
    "            loss_hist.append(loss)\n",
    "        return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "\n",
    "X = np.array([\n",
    "    [2,3,4],\n",
    "    [1,2,2]\n",
    "])\n",
    "y = np.array([0,3])\n",
    "# LR.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22426fe8630>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "#sanity\n",
    "LR = LinearRegression()\n",
    "X = np.random.randn(10,10)\n",
    "y = np.random.randint(100, size=(10,))\n",
    "loss_hist = LR.fit(X, y, learning_rate=1e-2, verbose=False)\n",
    "plt.plot(loss_hist)\n",
    "\n",
    "# print('________expected_________')\n",
    "# LR_sk = sklearn.linear_model.LinearRegression()\n",
    "# LR.fit(X, y, learning_rate=1e-2, verbose=False)\n",
    "# LR_sk = sklearn.linear_model.LinearRegression()\n",
    "# LR_sk.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical gradient check\n",
    "\n",
    "Using Calculus method\n",
    "$$\n",
    "f(h) = \\frac{f(x+h) - f(x-h)}{2 h}\n",
    "$$\n",
    "\n",
    "- for more info refer to; http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_grad(f, x, h=.0001):\n",
    "    fx = f(x) #eval\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        mask = np.zeros_like(x)\n",
    "        mask[ix] = 1\n",
    "        Lp = ((f(x) + h) * mask).sum()\n",
    "        Lm = ((f(x) - h) * mask).sum()\n",
    "        grad[ix] = (Lp - Lm) /(2 * h)\n",
    "        print(grad[ix])\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
